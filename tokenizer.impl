
template<class TokenIdT>
Tokenizer<TokenIdT>::Tokenizer(bool _by_pref):
m_by_pref(_by_pref) {
	
}

template<class TokenIdT>
Tokenizer<TokenIdT>::~Tokenizer() {
	
}

template<class TokenIdT>
TokenIdT Tokenizer<TokenIdT>::getTokenId(const std::string &_token) {
	typename hashtable<std::string, TokenIdT>::iterator it = m_tokens.find(_token);
	if (it != m_tokens.end())
		return it->second;
	
	TokenIdT newid = m_tokens.size();
	m_tokens[_token] = newid;

	return newid;
}

template<class TokenIdT>
TokenIdT Tokenizer<TokenIdT>::getTokenIdConst(const std::string &_token) const {
	
	typename hashtable<std::string, TokenIdT>::const_iterator it = m_tokens.find(_token);
	if (it != m_tokens.end())
		return it->second;
	return 0;
}

template<class TokenIdT>
void Tokenizer<TokenIdT>::splitByDelims(const std::string &_text, std::vector<std::string> &_words) const  {
	
	boost::split(_words, _text, boost::is_any_of("., ;:	-(){}\"\'<>\n"));
}

template<class TokenIdT>
void Tokenizer<TokenIdT>::tokenizeText(const std::string &_text, TextRepr &_words) {
	
	if (m_by_pref) {
		tokenizeTextPrefixes(_text, _words);
		return;
	}
	
	std::string text (_text);
	toLowerUtf8(text);
	fix_utf8_string(text);
	
	
	std::vector<std::string> words;
	splitByDelims(text, words);
	
	for (int i = 0; i<words.size(); i++)
		if (words[i].size() != 0) {
				_words.push_back(getTokenId(words[i]));
		}
}

template<class TokenIdT>
void Tokenizer<TokenIdT>::tokenizeTextConst(const std::string &_text, TextRepr &_words) const {
	
	
	std::string text (_text);
	toLowerUtf8(text);
	fix_utf8_string(text);
	
	std::vector<std::string> words;
	splitByDelims(text, words);
	
	for (int i = 0; i<words.size(); i++)
		if (words[i].size() != 0) {
			TokenIdT id = getTokenIdConst(words[i]);
			if (id != 0)
				_words.push_back(id);
		}
	
}

template<class TokenIdT>
void Tokenizer<TokenIdT>::tokenizeTextPrefixes(const std::string &_str_query, TextRepr &_text) {
	
	std::string text (_str_query);
	toLowerUtf8(text);
	fix_utf8_string(text);
	
	std::vector<std::string> words;
	splitByDelims(text, words);
	
	for (int i = 0; i<words.size(); i++)
		if (words[i].size() != 0) {
			
			std::vector<std::string> prefixes;
			getPrefixesUtf8(words[i], prefixes);
			
			for (int j = 0; j<prefixes.size(); j++)
				_text.push_back(getTokenId(prefixes[j]));
		}
}
/*
void Tokenizer::tokenizeDoc(uint64_t _id, const std::string &_text, Doc &_doc) {
	
	//std::string title(_title);
	std::string text(_text);
		
	tokenizeText(text, _doc.text);
	//tokenizeText(title, _doc.title);
	_doc.id = _id;
}*/
